{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea1250b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:47:21.677157Z",
     "start_time": "2023-06-13T01:47:21.303106Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from readmegetter import ReadmeGetter\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77c865c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:47:36.745926Z",
     "start_time": "2023-06-13T01:47:21.677157Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caell\\Documents\\data_science\\coding_dojo\\chatgpt-resume-builder\\readmegetter.py:37: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 37 of the file C:\\Users\\caell\\Documents\\data_science\\coding_dojo\\chatgpt-resume-builder\\readmegetter.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = bs4.BeautifulSoup(response.content)\n",
      "C:\\Users\\caell\\Documents\\data_science\\coding_dojo\\chatgpt-resume-builder\\readmegetter.py:17: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 17 of the file C:\\Users\\caell\\Documents\\data_science\\coding_dojo\\chatgpt-resume-builder\\readmegetter.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = bs4.BeautifulSoup(response.content)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Caellwyn/ou_student_predictions', 'Caellwyn/Seattle-Home-Sales', 'Violet-Spiral/covid-xprize', 'Caellwyn/product-flexible-twitter-sentiment-analysis', 'Caellwyn/pet-predictor', 'learn-co-curriculum/streamlit-image-classifier-demo', 'Caellwyn/chat-with-a-philosopher'])\n"
     ]
    }
   ],
   "source": [
    "readmegetter = ReadmeGetter()\n",
    "readmegetter.get_repos('https://github.com/Caellwyn')\n",
    "readmegetter.get_readmes()\n",
    "\n",
    "print(readmegetter.READMES.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264e46f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:48:37.396288Z",
     "start_time": "2023-06-13T01:48:37.384651Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_summaries(readme_dict):\n",
    "    summaries = {}\n",
    "    prefix = \"\"\"Please summarize the project readme following the colon into 3-5 bullet points\n",
    "    suitable for a resume that would impress a data science hiring manager.\n",
    "    The bullet points should highlight the skills necessary to complete the project:\"\"\"\n",
    "\n",
    "    for key, readme in readmegetter.READMES.items():\n",
    "        trunc_readme = readme[:4097] if len(readme) > 4097 else readme\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model = 'gpt-3.5-turbo',\n",
    "            messages=[{'role':'user', 'content':prefix + trunc_readme}]\n",
    "            )\n",
    "        summaries[key] = response.choices[0].message.content\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c2ae3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:49:15.030669Z",
     "start_time": "2023-06-13T01:48:38.120392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Developed a predictive model using data analytics to help identify students in danger of failing or withdrawing early in self-paced online learning environments\n",
      "- Utilized a dataset from Open University containing anonymized data from seven courses in social sciences and STEM fields from 2012/2013 and 2013/2014 academic years\n",
      "- Focused on student interaction data with the learning environment, such as activity statistics, to capture patterns that predict student success\n",
      "- Implemented shallow and deep model development, and collected various model candidates for validation on the latest cohorts in the dataset\n",
      "- Generated a report notebook presenting exploratory data analysis, diverse charts, and insights on the student behavior-success relationship, while providing a working environment and a presentation on the project's findings.\n",
      "- Demonstrated proficiency in data wrangling, exploration, and modeling through a project on Seattle Home Sales as part of the IBM Data Science professional certificate\n",
      "- Utilized data on properties and nearby amenities to estimate home prices, showcasing expertise in data analysis and modeling\n",
      "- Successfully created 3 sequential notebooks for Data Wrangling, Exploration, and Modeling, displaying strong organizational and technical skills\n",
      "- Applied linear regression models to determine optimal modeling for sales data and analyzed results, demonstrating critical thinking and problem-solving abilities\n",
      "- Developed and implemented models (SARIMAX and Facebook Prophet) to forecast infection rates based on historical data, achieving 84% and 99% accuracy, respectively.\n",
      "- Explored the impact of government interventions on the model's accuracy, but discovered potential issues with causation reversal.\n",
      "- Utilized techniques like seasonality and differencing to account for trends in the data and improve stationarity, leading to more valid predictions.\n",
      "- Validated the model's performance on US data from March to December 2020, providing a rigorous test of its effectiveness during a time of significant increases in case rates.\n",
      "- Developed a model to determine sentiment of products or brands based on tweets, with potential for driving production, inventory management, and providing insights to influencers and news organizations\n",
      "- Overcame challenges in processing data due to Twitterspeak and contextual relativity of emotions, with success in deciphering tweets and achieving over 3/5 classification accuracy using various models from recurrent neural networks to logistic regression\n",
      "- Trained model using four datasets from Crowdflower, including Brands and Product Emotions, Apple Twitter Sentiment, Deflategate Sentiment, and Coachella-2015, and conducted data exploration to derive insights from three columns with categorical values: Tweet Text, Tweet Target, and Tweet Sentiment.\n",
      "- Demonstrated deployment using Streamlit\n",
      "- Created a template prediction website that allows users to upload an image and get model predictions\n",
      "- Developed a model class in src.py that can be replaced with a user's own model\n",
      "- Used Procfile, setup.sh, and requirements.txt to properly configure the Streamlit server for deployment to Heroku\n",
      "- Used TensorFlow-CPU and downloaded pretrained weights at runtime to reduce slug size during deployment\n",
      "- Demonstrated deployment skills using Streamlit\n",
      "- Created a prediction website that allows users to upload images and returns model predictions\n",
      "- Developed a model class for modeling work and loading a pretrained model from saved weights\n",
      "- Configured deployment requirements including Procfile, setup.sh, and requirements.txt\n",
      "- Deployed project on Heroku, optimized slug size to fit within free account limits, and tested locally before deploying\n",
      "- Developed a ChatGPT-powered project that allows users to chat with historical philosophers such as Aristotle and Confucius through a Forum of Wisdom.\n",
      "- Utilized natural language processing and sentiment analysis skills to create a realistic and engaging chatbot experience.\n",
      "- Demonstrated proficiency in web development and deployment by making the project accessible online.\n",
      "- Showcased creativity, innovation, and a passion for philosophy through the project's unique and thought-provoking concept. \n",
      "- Proved an ability to integrate various technologies and tools to produce a functional and user-friendly application.\n"
     ]
    }
   ],
   "source": [
    "summaries = get_summaries(readmegetter.READMES)\n",
    "for key, summary in summaries.items():\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a983c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
