{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca666bd-6bdc-431e-9d4b-42e4ca2e30b6",
   "metadata": {},
   "source": [
    "# Scraping READMEs from GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2ef73-73be-476b-9f4a-6eee278660d2",
   "metadata": {},
   "source": [
    "## Approach\n",
    "- Two approaches discussed:\n",
    "    - MVP: accept a list of project repos to extract the READMEs from\n",
    "    - AAB: start with the user's GitHub profile and use the pinned repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5b48a8e-3335-4e87-9cdb-cd899aadc2d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T00:25:49.059489Z",
     "start_time": "2023-06-13T00:25:49.039543Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.parse import urljoin\n",
    "import requests, bs4, time\n",
    "\n",
    "class ReadmeGetter:\n",
    "    def __init__(self):\n",
    "        self.READMES = {}\n",
    "        self.repo_links = []\n",
    "    \n",
    "    def get_readmes(self):\n",
    "        # Gather readmes.  Must be used AFTER get_repos()\n",
    "        for repo in self.repo_links:\n",
    "\n",
    "            # get response\n",
    "            response = requests.get(repo)\n",
    "            # make a beautiful soup\n",
    "            soup = bs4.BeautifulSoup(response.content)\n",
    "\n",
    "            articles = soup.find_all('article')\n",
    "            try:\n",
    "                readme = articles[0]\n",
    "\n",
    "                key = repo.replace(base_url+'/','')#.replace('/','')\n",
    "                self.READMES[key] = readme.text\n",
    "        \n",
    "            except Exception as e:\n",
    "                display(e)\n",
    "                print(repo)\n",
    "            sec_sleep = np.random.choice([1.9,1.2, 1.34,1.1,0.9])\n",
    "            time.sleep(sec_sleep)\n",
    "    \n",
    "    def get_repos(self, profile_link):\n",
    "        # Takes a GitHub profile link and gathers links to all pinned repos\n",
    "        class_=\"js-pinned-items-reorder-container\"\n",
    "        \n",
    "        response = requests.get(profile)\n",
    "        soup = bs4.BeautifulSoup(response.content)\n",
    "        pins  = soup.find_all(class_=class_)\n",
    "        pinned_repos = pins[0]\n",
    "        links = pinned_repos.find_all('a', href=True)\n",
    "        base_url = \"https://www.github.com\"\n",
    "        repo_links = []\n",
    "\n",
    "        # saving list of absolute links\n",
    "\n",
    "        for link in links:\n",
    "            # relative link\n",
    "            rel_link = link['href']\n",
    "            abs_link = urljoin(base_url,rel_link)\n",
    "\n",
    "            # remove stars and forks\n",
    "            if abs_link.endswith('stargazers') | abs_link.endswith('forks'):\n",
    "                pass\n",
    "            else:\n",
    "                self.repo_links.append(abs_link)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c2e6d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T00:25:49.356050Z",
     "start_time": "2023-06-13T00:25:49.351062Z"
    }
   },
   "outputs": [],
   "source": [
    "Getter = ReadmeGetter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49fbcc1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T00:25:50.909172Z",
     "start_time": "2023-06-13T00:25:50.030972Z"
    }
   },
   "outputs": [],
   "source": [
    "Getter.get_repos('https://github.com/Caellwyn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a02aaf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T00:25:52.686991Z",
     "start_time": "2023-06-13T00:25:52.671033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.github.com/Caellwyn/ou_student_predictions',\n",
       " 'https://www.github.com/Caellwyn/Seattle-Home-Sales',\n",
       " 'https://www.github.com/Violet-Spiral/covid-xprize',\n",
       " 'https://www.github.com/Caellwyn/product-flexible-twitter-sentiment-analysis',\n",
       " 'https://www.github.com/Caellwyn/pet-predictor',\n",
       " 'https://www.github.com/learn-co-curriculum/streamlit-image-classifier-demo',\n",
       " 'https://www.github.com/Caellwyn/chat-with-a-philosopher']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Getter.repo_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cde6dcd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T00:26:07.429580Z",
     "start_time": "2023-06-13T00:25:53.167038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Caellwyn/ou_student_predictions', 'Caellwyn/Seattle-Home-Sales', 'Violet-Spiral/covid-xprize', 'Caellwyn/product-flexible-twitter-sentiment-analysis', 'Caellwyn/pet-predictor', 'learn-co-curriculum/streamlit-image-classifier-demo', 'Caellwyn/chat-with-a-philosopher'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Getter.get_readmes()\n",
    "Getter.READMES.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37363026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T00:27:24.067394Z",
     "start_time": "2023-06-13T00:27:24.047474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seattle Home Sales\\nData Science Projects worthy of sharing\\nThis is a project that I created for the capstone class of the IBM Data Science professional certificate from Coursera in March 2020.  It explores the Seattle housing market and tries to estimate home prices using data on the properties and data about nearby services.\\nThe notebooks for this project are split into 3 sequential pieces.\\n\\nData Wrangling: where I collect and format my dataset\\nData Exploration: where I explore, map, and graph correlations within the dataset.\\nData Modeling: where I test several linear regression models to determine how best to model the sales data and analyze the results.\\n\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Getter.READMES[list(Getter.READMES.keys())[1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
