{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca666bd-6bdc-431e-9d4b-42e4ca2e30b6",
   "metadata": {},
   "source": [
    "# Scraping READMEs from GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2ef73-73be-476b-9f4a-6eee278660d2",
   "metadata": {},
   "source": [
    "## Approach\n",
    "- Two approaches discussed:\n",
    "    - MVP: accept a list of project repos to extract the READMEs from\n",
    "    - AAB: start with the user's GitHub profile and use the pinned repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7251b77-86c8-4bcc-ab02-652e8521b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83552a94-79eb-4684-8b27-bf9d442c0c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9fca8de-5dc2-434a-8f5f-3cc40c494184",
   "metadata": {},
   "source": [
    "# MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93dc6873-7a8b-4d1e-8d28-cb709d54e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MVP approach\n",
    "repos = ['https://github.com/jirvingphd/how-to-make-successful-movies',\n",
    "         'https://github.com/jirvingphd/how-to-spot-a-russian-troll-tweet-mod-4-project'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f35194-f811-4e43-95af-2fc024f2d2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/jirvingphd/how-to-make-successful-movies'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select test repo\n",
    "repo = repos[0]\n",
    "repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a30e544-bfd8-4ff6-85c5-55a8312f6db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get response\n",
    "response = requests.get(repo)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "962fa0a7-e091-4cff-b662-255ebe3606fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a beautiful soup\n",
    "soup = bs4.BeautifulSoup(response.content)\n",
    "len(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ea956-3458-4dc7-910a-e0e7165702ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3d6e236-21e5-4abe-bcb7-24260b07e0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to Make a Successful Movie\n",
      "Business Problem\n",
      "Specifications/Constraints\n",
      "Part 1- Initial IMDB Data Processing.ipynb\n",
      "IMDB Movie Metadata\n",
      "Part 2 - Extracting TMDB Data.ipynb\n",
      "Supplement Data from The Movie Database  (TMDB)'s\n",
      "EDA Summary of Extracted Data\n",
      "Years Extracted (thus far)\n",
      "MPAA Rating Counts\n",
      "MPAA Rating Revenue Comparison\n",
      "MPAA Rating - Average Budget Comparison\n",
      "MPAA Rating - Average ROI Comparison\n",
      "Part 3 - MySQL Database Construction\n",
      "ERD\n",
      "Part 4 - Hypothesis Testing - WIP\n",
      "Q1: Do some MPAA Ratings make more revenue than others?\n",
      "Hypothesis\n",
      "Selecting the Right Test\n",
      "ANOVA Assumptions\n",
      "Outliers Removed\n",
      "Normality Assumption\n",
      "Final Conclusion\n",
      "Future Work: Planned Hypotheses to Test\n",
      "Part 5 - Regression Model-Based Insights - WIP\n",
      "Best Model\n",
      "Regression Model Coefiicents and Importances\n",
      "OLS Coefficients\n",
      "Random Forest - Built-in Feature Importance\n",
      "Permutation Importance\n",
      "Part 6 - Classification Model-Based Insights - WIP\n",
      "Random Forest Classifier - BuiltIn Feature  Importances\n",
      "Random Forest Classifier -  Permutation Improtances\n",
      "Random Forest - SHAP Summary Plot\n",
      "Summary\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "README.md\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to Make a Successful Movie\n",
      "\n",
      "James M. Irving\n",
      "\n",
      "\n",
      "Business Problem\n",
      "I have been hired to process and analyze IMDB's extensive publicly-available dataset, supplement it with financial data from TMDB's API, convert the raw data into a MySQL database, and then use that database for extracting insights and recommendations on how to make a successful movie.\n",
      "I will use a combination of machine-learning-model-based insights and hypothesis testing to extract insights for our stakeholders.\n",
      "Specifications/Constraints\n",
      "\n",
      "The stakeholder wants to focus on attributes of the movies themselves vs. the actors and directors connected to those movies.\n",
      "They only want to include information related to movies released in the United States.\n",
      "They also did not want to include movies released before the year 2000.\n",
      "The stakeholder is particularly interested in how the MPAA rating, genre(s), runtime, budget, and production companies influence movie revenue and user ratings.\n",
      "\n",
      "Part 1- Initial IMDB Data Processing.ipynb\n",
      "IMDB Movie Metadata\n",
      "\n",
      "\n",
      "I will download fresh movie metadata from IMDB's public datasets and filter out movies that meet the stakeholder's requirements/constraints.\n",
      "\n",
      "\n",
      "IMDB Provides Several Files with varied information for Movies, TV Shows, Made for TV Movies, etc.\n",
      "\n",
      "\n",
      "Overview/Data Dictionary: https://www.imdb.com/interfaces/\n",
      "\n",
      "\n",
      "Downloads page: https://datasets.imdbws.com/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Files to use:\n",
      "\n",
      "title.basics.tsv.gz\n",
      "title.ratings.tsv.gz\n",
      "title.akas.tsv.gz\n",
      "\n",
      "\n",
      "\n",
      "Part 2 - Extracting TMDB Data.ipynb\n",
      "Supplement Data from The Movie Database  (TMDB)'s\n",
      "\n",
      "I will extract MPAA rating and financial data for the movies using TMDB's API.\n",
      "\n",
      "\n",
      "\n",
      "\"This product uses the TMDB API but is not endorsed or certified by TMDB.\"\n",
      "\n",
      "EDA Summary of Extracted Data\n",
      "Years Extracted (thus far)\n",
      "\n",
      "MPAA Rating Counts\n",
      "\n",
      "MPAA Rating Revenue Comparison\n",
      "\n",
      "MPAA Rating - Average Budget Comparison\n",
      "\n",
      "MPAA Rating - Average ROI Comparison\n",
      "\n",
      "Part 3 - MySQL Database Construction\n",
      "\n",
      "I will then normalize all IMDB movie data into a proper MySQL database.\n",
      "\n",
      "MVP Version (included): Local Server Installation with Publicly-Available .sql file for recreationl.\n",
      "AAB Version (future work): AWS-hosted RDS MySQL database.\n",
      "\n",
      "\n",
      "See SQL folder for:\n",
      "\n",
      "movies.sql: exported SQL DB\n",
      "movies_project.mwb: model used for ERD\n",
      "\n",
      "\n",
      "\n",
      "ERD\n",
      "\n",
      "Part 4 - Hypothesis Testing - WIP\n",
      "\n",
      "I will then use the MySQL database to answer several hypotheses about movie success.\n",
      "\n",
      "Q1: Do some MPAA Ratings make more revenue than others?\n",
      "Hypothesis\n",
      "\n",
      "\n",
      "$H_0$ (Null Hypothesis): All MPAA ratings generate have equal average revenue.\n",
      "\n",
      "$H_A$ (Alternative Hypothesis):  Some MPAA ratings earn significantly more/less revenue than others.\n",
      "\n",
      "\n",
      "Selecting the Right Test\n",
      "\n",
      "We have Numerical Data, with more than 2 groups, and therefore want to perform One way ANOVA.\n",
      "\n",
      "\n",
      "ANOVA Assumptions\n",
      "\n",
      "No significant outliers\n",
      "Normality\n",
      "Equal Variance\n",
      "\n",
      "\n",
      "Outliers Removed\n",
      "\n",
      "There were 2 outliers in the PG-13 group.\n",
      "There were 1 outliers in the PG group.\n",
      "There were 4 outliers in the R group.\n",
      "There were 1 outliers in the G group.\n",
      "\n",
      "\n",
      "Normality Assumption\n",
      "\n",
      "We failed a Shapiro's test for normality for several groups and had too small $n$ for G-rated movies to safely ignore asumpton.\n",
      "\n",
      "\n",
      "\n",
      "Therefore, we will perform a Kruskal-Wallis test instead of a One Way ANOVA\n",
      "\n",
      "\n",
      "Final Conclusion\n",
      "\n",
      "Test Result: KruskalResult(statistic=29.15915632270992, pvalue=2.0734380490347713e-06)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Our Kruskal Wallis test returned a p-value <.0001. We reject the null hypothesis.\n",
      "- There is a significant difference in the average revenue for different movie certifications.\n",
      "- A post-hoc determined that movies rated R made significantly less than all other raings.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Future Work: Planned Hypotheses to Test\n",
      "\n",
      "\n",
      " Q1: Does the MPAA rating of a movie (G/PG/PG-13/R) affect how much revenue the movie generates? If so, which rating earns the most revenue?\n",
      "\n",
      "\n",
      "Q1B: What if compare ROI instead of revenue?\n",
      "\n",
      "\n",
      "Q2; Do movies that are over 2.5 hours long earn more revenue than movies that are 1.5 hours long (or less)?\n",
      "\n",
      "\n",
      "Q3: Do movies released in 2020 earn less revenue than movies released in 2018?\n",
      "\n",
      "\n",
      "Q4: Do some movie genres earn more revenue than others?\n",
      "\n",
      "\n",
      "Q5: Are some genres more highly rated than others?\n",
      "\n",
      "\n",
      "\n",
      "Part 5 - Regression Model-Based Insights - WIP\n",
      "\n",
      "Finally I will use Linear Regression and other machine learning models to predict movie revenue / ROI to extract insights and recommendations on what features of a movie are positive/negative predictors of success.\n",
      "\n",
      "\n",
      "Best Model\n",
      "OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                revenue   R-squared:                       0.637\n",
      "Model:                            OLS   Adj. R-squared:                  0.630\n",
      "Method:                 Least Squares   F-statistic:                     85.56\n",
      "Date:                Sat, 21 May 2022   Prob (F-statistic):               0.00\n",
      "Time:                        22:28:25   Log-Likelihood:                -60226.\n",
      "No. Observations:                3229   AIC:                         1.206e+05\n",
      "Df Residuals:                    3163   BIC:                         1.210e+05\n",
      "Df Model:                          65                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================================\n",
      "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------\n",
      "certification_G                    -5.013e+06   3.09e+07     -0.162      0.871   -6.55e+07    5.55e+07\n",
      "certification_MISSING                  0.6335      0.028     22.276      0.000       0.578       0.689\n",
      "certification_NC-17                 5.324e+05   6.58e+04      8.085      0.000    4.03e+05    6.61e+05\n",
      "certification_NR                    1.255e+05   2.88e+04      4.363      0.000    6.91e+04    1.82e+05\n",
      "certification_PG                   -1.088e+05   5.19e+05     -0.210      0.834   -1.13e+06    9.09e+05\n",
      "certification_PG-13                 1.218e+04    473.072     25.747      0.000    1.13e+04    1.31e+04\n",
      "certification_R                    -2.189e+05   1.07e+05     -2.038      0.042    -4.3e+05   -8310.648\n",
      "adult                              -1.054e+05   1.59e+05     -0.662      0.508   -4.17e+05    2.07e+05\n",
      "budget                              2.341e+04    6.4e+04      0.366      0.715   -1.02e+05    1.49e+05\n",
      "popularity                          6.195e+07   2.72e+07      2.278      0.023    8.62e+06    1.15e+08\n",
      "runtime                             5.291e+07   2.72e+07      1.942      0.052   -5.19e+05    1.06e+08\n",
      "vote_average                        4.577e+07    2.8e+07      1.637      0.102   -9.04e+06    1.01e+08\n",
      "vote_count                          5.081e+07   2.73e+07      1.863      0.063   -2.66e+06    1.04e+08\n",
      "year                                 6.01e+07   2.71e+07      2.219      0.027       7e+06    1.13e+08\n",
      "month                               5.592e+07    2.7e+07      2.074      0.038    3.05e+06    1.09e+08\n",
      "day                                 4.456e+07   2.69e+07      1.654      0.098   -8.26e+06    9.74e+07\n",
      "belongs_to_collection                1.67e+07   1.58e+06     10.563      0.000    1.36e+07    1.98e+07\n",
      "ProdComp_20th Century Fox           9.646e+06   3.92e+06      2.458      0.014    1.95e+06    1.73e+07\n",
      "ProdComp_BBC Films                  9.638e+05   5.09e+06      0.189      0.850   -9.01e+06    1.09e+07\n",
      "ProdComp_Blumhouse Productions      1.456e+07   5.95e+06      2.447      0.014    2.89e+06    2.62e+07\n",
      "ProdComp_CJ Entertainment             1.6e+07      6e+06      2.664      0.008    4.22e+06    2.78e+07\n",
      "ProdComp_Canal+                     4.215e+05   3.86e+06      0.109      0.913   -7.14e+06    7.98e+06\n",
      "ProdComp_Ciné+                       -6.7e+06    6.4e+06     -1.047      0.295   -1.92e+07    5.84e+06\n",
      "ProdComp_Columbia Pictures          2.052e+07   3.24e+06      6.334      0.000    1.42e+07    2.69e+07\n",
      "ProdComp_Dimension Films            5.014e+06   5.36e+06      0.936      0.349   -5.49e+06    1.55e+07\n",
      "ProdComp_DreamWorks Animation       4.437e+07    1.4e+07      3.160      0.002    1.68e+07    7.19e+07\n",
      "ProdComp_DreamWorks Pictures        9.409e+06    4.3e+06      2.188      0.029    9.77e+05    1.78e+07\n",
      "ProdComp_Dune Entertainment         1.419e+07   5.58e+06      2.546      0.011    3.26e+06    2.51e+07\n",
      "ProdComp_Epsilon Motion Pictures   -7.884e+06   6.33e+06     -1.245      0.213   -2.03e+07    4.53e+06\n",
      "ProdComp_EuropaCorp                -1.165e+07   6.67e+06     -1.745      0.081   -2.47e+07    1.44e+06\n",
      "ProdComp_Film4 Productions         -6.472e+06    5.5e+06     -1.176      0.240   -1.73e+07    4.32e+06\n",
      "ProdComp_Focus Features             2.026e+06   4.48e+06      0.452      0.651   -6.76e+06    1.08e+07\n",
      "ProdComp_Fox 2000 Pictures          1.651e+07   6.93e+06      2.381      0.017    2.92e+06    3.01e+07\n",
      "ProdComp_Fox Searchlight Pictures  -2.021e+06   4.81e+06     -0.420      0.675   -1.15e+07    7.42e+06\n",
      "ProdComp_France 2 Cinéma           -1.154e+07    6.2e+06     -1.860      0.063   -2.37e+07    6.25e+05\n",
      "ProdComp_France 3 Cinéma           -1.394e+06   6.02e+06     -0.232      0.817   -1.32e+07    1.04e+07\n",
      "ProdComp_Ingenious Media           -5.116e+06   6.49e+06     -0.788      0.431   -1.78e+07    7.61e+06\n",
      "ProdComp_Legendary Pictures        -5.125e+06   8.51e+06     -0.603      0.547   -2.18e+07    1.16e+07\n",
      "ProdComp_Lionsgate                 -5.177e+05   3.62e+06     -0.143      0.886   -7.61e+06    6.57e+06\n",
      "ProdComp_MISSING                     3.23e-09   6.99e-09      0.462      0.644   -1.05e-08    1.69e-08\n",
      "ProdComp_Metro-Goldwyn-Mayer        6.656e+06   4.56e+06      1.459      0.145   -2.29e+06    1.56e+07\n",
      "ProdComp_Millennium Films          -6.043e+06   5.23e+06     -1.155      0.248   -1.63e+07    4.22e+06\n",
      "ProdComp_Miramax                    1.041e+07   4.57e+06      2.280      0.023    1.46e+06    1.94e+07\n",
      "ProdComp_New Line Cinema            1.999e+07   3.62e+06      5.522      0.000    1.29e+07    2.71e+07\n",
      "ProdComp_New Regency Pictures       -4.61e+05   8.07e+06     -0.057      0.954   -1.63e+07    1.54e+07\n",
      "ProdComp_Original Film             -3.267e+06   6.88e+06     -0.475      0.635   -1.68e+07    1.02e+07\n",
      "ProdComp_Paramount                  1.899e+07   3.22e+06      5.902      0.000    1.27e+07    2.53e+07\n",
      "ProdComp_Participant                4.634e+06   6.27e+06      0.739      0.460   -7.66e+06    1.69e+07\n",
      "ProdComp_Regency Enterprises       -1.741e+06   7.34e+06     -0.237      0.812   -1.61e+07    1.26e+07\n",
      "ProdComp_Relativity Media            9.11e+06   4.07e+06      2.239      0.025    1.13e+06    1.71e+07\n",
      "ProdComp_Revolution Studios        -1.358e+06    6.2e+06     -0.219      0.827   -1.35e+07    1.08e+07\n",
      "ProdComp_Scott Free Productions    -3.074e+06   6.64e+06     -0.463      0.643   -1.61e+07    9.94e+06\n",
      "ProdComp_Scott Rudin Productions    3.177e+05   5.52e+06      0.058      0.954   -1.05e+07    1.11e+07\n",
      "ProdComp_Screen Gems                1.468e+07   4.62e+06      3.175      0.002    5.61e+06    2.37e+07\n",
      "ProdComp_Sony Pictures              8.084e+06   4.65e+06      1.740      0.082   -1.02e+06    1.72e+07\n",
      "ProdComp_Spyglass Entertainment      5.08e+06   6.77e+06      0.751      0.453   -8.19e+06    1.83e+07\n",
      "ProdComp_StudioCanal                5.524e+06   4.35e+06      1.269      0.205   -3.01e+06    1.41e+07\n",
      "ProdComp_Summit Entertainment      -2.763e+06   4.66e+06     -0.593      0.553   -1.19e+07    6.38e+06\n",
      "ProdComp_TF1 Films Production       5.961e+06   7.19e+06      0.829      0.407   -8.14e+06    2.01e+07\n",
      "ProdComp_TSG Entertainment          9.122e+05   7.01e+06      0.130      0.896   -1.28e+07    1.46e+07\n",
      "ProdComp_The Weinstein Company     -9.314e+05   5.49e+06     -0.170      0.865   -1.17e+07    9.83e+06\n",
      "ProdComp_Touchstone Pictures         1.59e+07   4.57e+06      3.483      0.001    6.95e+06    2.49e+07\n",
      "ProdComp_Universal Pictures         1.381e+07   2.82e+06      4.892      0.000    8.28e+06    1.93e+07\n",
      "ProdComp_Village Roadshow Pictures  7.873e+06   5.63e+06      1.399      0.162   -3.16e+06    1.89e+07\n",
      "ProdComp_Walt Disney Pictures       2.679e+07   4.66e+06      5.751      0.000    1.77e+07    3.59e+07\n",
      "ProdComp_Warner Bros. Pictures      1.684e+06   3.16e+06      0.533      0.594   -4.51e+06    7.88e+06\n",
      "ProdComp_Working Title Films        1.444e+06   6.35e+06      0.227      0.820    -1.1e+07    1.39e+07\n",
      "const                                3.72e+08   1.89e+08      1.969      0.049    1.65e+06    7.42e+08\n",
      "==============================================================================\n",
      "Omnibus:                      606.983   Durbin-Watson:                   1.992\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2441.537\n",
      "Skew:                           0.875   Prob(JB):                         0.00\n",
      "Kurtosis:                       6.884   Cond. No.                     2.54e+23\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.63e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "\n",
      "\n",
      "Regression Model Coefiicents and Importances\n",
      "\n",
      "OLS Coefficients\n",
      "\n",
      "\n",
      "Random Forest - Built-in Feature Importance\n",
      "\n",
      "\n",
      "Permutation Importance\n",
      "\n",
      "\n",
      "Part 6 - Classification Model-Based Insights - WIP\n",
      "\n",
      "Random Forest Classifier - BuiltIn Feature  Importances\n",
      "\n",
      "\n",
      "Random Forest Classifier -  Permutation Improtances\n",
      "\n",
      "\n",
      "Random Forest - SHAP Summary Plot\n",
      "\n",
      "\n",
      "Summary\n",
      "\n",
      "Coming soon!\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the readme \n",
    "tag = soup.find_all(id='readme')\n",
    "readme = tag[0]\n",
    "print(readme.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9844602d-cd2e-4ff9-a8af-ae554582bd53",
   "metadata": {},
   "source": [
    "- Too big of a result. Includes the toc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "275d59ae-a707-4287-acc3-0027423dfc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = soup.find_all('article')\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d604ddf4-894c-4720-ab8a-ab6ef5dc89ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Make a Successful Movie\n",
      "\n",
      "James M. Irving\n",
      "\n",
      "\n",
      "Business Problem\n",
      "I have been hired to process and analyze IMDB's extensive publicly-available dataset, supplement it with financial data from TMDB's API, convert the raw data into a MySQL database, and then use that database for extracting insights and recommendations on how to make a successful movie.\n",
      "I will use a combination of machine-learning-model-based insights and hypothesis testing to extract insights for our stakeholders.\n",
      "Specifications/Constraints\n",
      "\n",
      "The stakeholder wants to focus on attributes of the movies themselves vs. the actors and directors connected to those movies.\n",
      "They only want to include information related to movies released in the United States.\n",
      "They also did not want to include movies released before the year 2000.\n",
      "The stakeholder is particularly interested in how the MPAA rating, genre(s), runtime, budget, and production companies influence movie revenue and user ratings.\n",
      "\n",
      "Part 1- Initial IMDB Data Processing.ipynb\n",
      "IMDB Movie Metadata\n",
      "\n",
      "\n",
      "I will download fresh movie metadata from IMDB's public datasets and filter out movies that meet the stakeholder's requirements/constraints.\n",
      "\n",
      "\n",
      "IMDB Provides Several Files with varied information for Movies, TV Shows, Made for TV Movies, etc.\n",
      "\n",
      "\n",
      "Overview/Data Dictionary: https://www.imdb.com/interfaces/\n",
      "\n",
      "\n",
      "Downloads page: https://datasets.imdbws.com/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Files to use:\n",
      "\n",
      "title.basics.tsv.gz\n",
      "title.ratings.tsv.gz\n",
      "title.akas.tsv.gz\n",
      "\n",
      "\n",
      "\n",
      "Part 2 - Extracting TMDB Data.ipynb\n",
      "Supplement Data from The Movie Database  (TMDB)'s\n",
      "\n",
      "I will extract MPAA rating and financial data for the movies using TMDB's API.\n",
      "\n",
      "\n",
      "\n",
      "\"This product uses the TMDB API but is not endorsed or certified by TMDB.\"\n",
      "\n",
      "EDA Summary of Extracted Data\n",
      "Years Extracted (thus far)\n",
      "\n",
      "MPAA Rating Counts\n",
      "\n",
      "MPAA Rating Revenue Comparison\n",
      "\n",
      "MPAA Rating - Average Budget Comparison\n",
      "\n",
      "MPAA Rating - Average ROI Comparison\n",
      "\n",
      "Part 3 - MySQL Database Construction\n",
      "\n",
      "I will then normalize all IMDB movie data into a proper MySQL database.\n",
      "\n",
      "MVP Version (included): Local Server Installation with Publicly-Available .sql file for recreationl.\n",
      "AAB Version (future work): AWS-hosted RDS MySQL database.\n",
      "\n",
      "\n",
      "See SQL folder for:\n",
      "\n",
      "movies.sql: exported SQL DB\n",
      "movies_project.mwb: model used for ERD\n",
      "\n",
      "\n",
      "\n",
      "ERD\n",
      "\n",
      "Part 4 - Hypothesis Testing - WIP\n",
      "\n",
      "I will then use the MySQL database to answer several hypotheses about movie success.\n",
      "\n",
      "Q1: Do some MPAA Ratings make more revenue than others?\n",
      "Hypothesis\n",
      "\n",
      "\n",
      "$H_0$ (Null Hypothesis): All MPAA ratings generate have equal average revenue.\n",
      "\n",
      "$H_A$ (Alternative Hypothesis):  Some MPAA ratings earn significantly more/less revenue than others.\n",
      "\n",
      "\n",
      "Selecting the Right Test\n",
      "\n",
      "We have Numerical Data, with more than 2 groups, and therefore want to perform One way ANOVA.\n",
      "\n",
      "\n",
      "ANOVA Assumptions\n",
      "\n",
      "No significant outliers\n",
      "Normality\n",
      "Equal Variance\n",
      "\n",
      "\n",
      "Outliers Removed\n",
      "\n",
      "There were 2 outliers in the PG-13 group.\n",
      "There were 1 outliers in the PG group.\n",
      "There were 4 outliers in the R group.\n",
      "There were 1 outliers in the G group.\n",
      "\n",
      "\n",
      "Normality Assumption\n",
      "\n",
      "We failed a Shapiro's test for normality for several groups and had too small $n$ for G-rated movies to safely ignore asumpton.\n",
      "\n",
      "\n",
      "\n",
      "Therefore, we will perform a Kruskal-Wallis test instead of a One Way ANOVA\n",
      "\n",
      "\n",
      "Final Conclusion\n",
      "\n",
      "Test Result: KruskalResult(statistic=29.15915632270992, pvalue=2.0734380490347713e-06)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Our Kruskal Wallis test returned a p-value <.0001. We reject the null hypothesis.\n",
      "- There is a significant difference in the average revenue for different movie certifications.\n",
      "- A post-hoc determined that movies rated R made significantly less than all other raings.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Future Work: Planned Hypotheses to Test\n",
      "\n",
      "\n",
      " Q1: Does the MPAA rating of a movie (G/PG/PG-13/R) affect how much revenue the movie generates? If so, which rating earns the most revenue?\n",
      "\n",
      "\n",
      "Q1B: What if compare ROI instead of revenue?\n",
      "\n",
      "\n",
      "Q2; Do movies that are over 2.5 hours long earn more revenue than movies that are 1.5 hours long (or less)?\n",
      "\n",
      "\n",
      "Q3: Do movies released in 2020 earn less revenue than movies released in 2018?\n",
      "\n",
      "\n",
      "Q4: Do some movie genres earn more revenue than others?\n",
      "\n",
      "\n",
      "Q5: Are some genres more highly rated than others?\n",
      "\n",
      "\n",
      "\n",
      "Part 5 - Regression Model-Based Insights - WIP\n",
      "\n",
      "Finally I will use Linear Regression and other machine learning models to predict movie revenue / ROI to extract insights and recommendations on what features of a movie are positive/negative predictors of success.\n",
      "\n",
      "\n",
      "Best Model\n",
      "OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                revenue   R-squared:                       0.637\n",
      "Model:                            OLS   Adj. R-squared:                  0.630\n",
      "Method:                 Least Squares   F-statistic:                     85.56\n",
      "Date:                Sat, 21 May 2022   Prob (F-statistic):               0.00\n",
      "Time:                        22:28:25   Log-Likelihood:                -60226.\n",
      "No. Observations:                3229   AIC:                         1.206e+05\n",
      "Df Residuals:                    3163   BIC:                         1.210e+05\n",
      "Df Model:                          65                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================================\n",
      "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------\n",
      "certification_G                    -5.013e+06   3.09e+07     -0.162      0.871   -6.55e+07    5.55e+07\n",
      "certification_MISSING                  0.6335      0.028     22.276      0.000       0.578       0.689\n",
      "certification_NC-17                 5.324e+05   6.58e+04      8.085      0.000    4.03e+05    6.61e+05\n",
      "certification_NR                    1.255e+05   2.88e+04      4.363      0.000    6.91e+04    1.82e+05\n",
      "certification_PG                   -1.088e+05   5.19e+05     -0.210      0.834   -1.13e+06    9.09e+05\n",
      "certification_PG-13                 1.218e+04    473.072     25.747      0.000    1.13e+04    1.31e+04\n",
      "certification_R                    -2.189e+05   1.07e+05     -2.038      0.042    -4.3e+05   -8310.648\n",
      "adult                              -1.054e+05   1.59e+05     -0.662      0.508   -4.17e+05    2.07e+05\n",
      "budget                              2.341e+04    6.4e+04      0.366      0.715   -1.02e+05    1.49e+05\n",
      "popularity                          6.195e+07   2.72e+07      2.278      0.023    8.62e+06    1.15e+08\n",
      "runtime                             5.291e+07   2.72e+07      1.942      0.052   -5.19e+05    1.06e+08\n",
      "vote_average                        4.577e+07    2.8e+07      1.637      0.102   -9.04e+06    1.01e+08\n",
      "vote_count                          5.081e+07   2.73e+07      1.863      0.063   -2.66e+06    1.04e+08\n",
      "year                                 6.01e+07   2.71e+07      2.219      0.027       7e+06    1.13e+08\n",
      "month                               5.592e+07    2.7e+07      2.074      0.038    3.05e+06    1.09e+08\n",
      "day                                 4.456e+07   2.69e+07      1.654      0.098   -8.26e+06    9.74e+07\n",
      "belongs_to_collection                1.67e+07   1.58e+06     10.563      0.000    1.36e+07    1.98e+07\n",
      "ProdComp_20th Century Fox           9.646e+06   3.92e+06      2.458      0.014    1.95e+06    1.73e+07\n",
      "ProdComp_BBC Films                  9.638e+05   5.09e+06      0.189      0.850   -9.01e+06    1.09e+07\n",
      "ProdComp_Blumhouse Productions      1.456e+07   5.95e+06      2.447      0.014    2.89e+06    2.62e+07\n",
      "ProdComp_CJ Entertainment             1.6e+07      6e+06      2.664      0.008    4.22e+06    2.78e+07\n",
      "ProdComp_Canal+                     4.215e+05   3.86e+06      0.109      0.913   -7.14e+06    7.98e+06\n",
      "ProdComp_Ciné+                       -6.7e+06    6.4e+06     -1.047      0.295   -1.92e+07    5.84e+06\n",
      "ProdComp_Columbia Pictures          2.052e+07   3.24e+06      6.334      0.000    1.42e+07    2.69e+07\n",
      "ProdComp_Dimension Films            5.014e+06   5.36e+06      0.936      0.349   -5.49e+06    1.55e+07\n",
      "ProdComp_DreamWorks Animation       4.437e+07    1.4e+07      3.160      0.002    1.68e+07    7.19e+07\n",
      "ProdComp_DreamWorks Pictures        9.409e+06    4.3e+06      2.188      0.029    9.77e+05    1.78e+07\n",
      "ProdComp_Dune Entertainment         1.419e+07   5.58e+06      2.546      0.011    3.26e+06    2.51e+07\n",
      "ProdComp_Epsilon Motion Pictures   -7.884e+06   6.33e+06     -1.245      0.213   -2.03e+07    4.53e+06\n",
      "ProdComp_EuropaCorp                -1.165e+07   6.67e+06     -1.745      0.081   -2.47e+07    1.44e+06\n",
      "ProdComp_Film4 Productions         -6.472e+06    5.5e+06     -1.176      0.240   -1.73e+07    4.32e+06\n",
      "ProdComp_Focus Features             2.026e+06   4.48e+06      0.452      0.651   -6.76e+06    1.08e+07\n",
      "ProdComp_Fox 2000 Pictures          1.651e+07   6.93e+06      2.381      0.017    2.92e+06    3.01e+07\n",
      "ProdComp_Fox Searchlight Pictures  -2.021e+06   4.81e+06     -0.420      0.675   -1.15e+07    7.42e+06\n",
      "ProdComp_France 2 Cinéma           -1.154e+07    6.2e+06     -1.860      0.063   -2.37e+07    6.25e+05\n",
      "ProdComp_France 3 Cinéma           -1.394e+06   6.02e+06     -0.232      0.817   -1.32e+07    1.04e+07\n",
      "ProdComp_Ingenious Media           -5.116e+06   6.49e+06     -0.788      0.431   -1.78e+07    7.61e+06\n",
      "ProdComp_Legendary Pictures        -5.125e+06   8.51e+06     -0.603      0.547   -2.18e+07    1.16e+07\n",
      "ProdComp_Lionsgate                 -5.177e+05   3.62e+06     -0.143      0.886   -7.61e+06    6.57e+06\n",
      "ProdComp_MISSING                     3.23e-09   6.99e-09      0.462      0.644   -1.05e-08    1.69e-08\n",
      "ProdComp_Metro-Goldwyn-Mayer        6.656e+06   4.56e+06      1.459      0.145   -2.29e+06    1.56e+07\n",
      "ProdComp_Millennium Films          -6.043e+06   5.23e+06     -1.155      0.248   -1.63e+07    4.22e+06\n",
      "ProdComp_Miramax                    1.041e+07   4.57e+06      2.280      0.023    1.46e+06    1.94e+07\n",
      "ProdComp_New Line Cinema            1.999e+07   3.62e+06      5.522      0.000    1.29e+07    2.71e+07\n",
      "ProdComp_New Regency Pictures       -4.61e+05   8.07e+06     -0.057      0.954   -1.63e+07    1.54e+07\n",
      "ProdComp_Original Film             -3.267e+06   6.88e+06     -0.475      0.635   -1.68e+07    1.02e+07\n",
      "ProdComp_Paramount                  1.899e+07   3.22e+06      5.902      0.000    1.27e+07    2.53e+07\n",
      "ProdComp_Participant                4.634e+06   6.27e+06      0.739      0.460   -7.66e+06    1.69e+07\n",
      "ProdComp_Regency Enterprises       -1.741e+06   7.34e+06     -0.237      0.812   -1.61e+07    1.26e+07\n",
      "ProdComp_Relativity Media            9.11e+06   4.07e+06      2.239      0.025    1.13e+06    1.71e+07\n",
      "ProdComp_Revolution Studios        -1.358e+06    6.2e+06     -0.219      0.827   -1.35e+07    1.08e+07\n",
      "ProdComp_Scott Free Productions    -3.074e+06   6.64e+06     -0.463      0.643   -1.61e+07    9.94e+06\n",
      "ProdComp_Scott Rudin Productions    3.177e+05   5.52e+06      0.058      0.954   -1.05e+07    1.11e+07\n",
      "ProdComp_Screen Gems                1.468e+07   4.62e+06      3.175      0.002    5.61e+06    2.37e+07\n",
      "ProdComp_Sony Pictures              8.084e+06   4.65e+06      1.740      0.082   -1.02e+06    1.72e+07\n",
      "ProdComp_Spyglass Entertainment      5.08e+06   6.77e+06      0.751      0.453   -8.19e+06    1.83e+07\n",
      "ProdComp_StudioCanal                5.524e+06   4.35e+06      1.269      0.205   -3.01e+06    1.41e+07\n",
      "ProdComp_Summit Entertainment      -2.763e+06   4.66e+06     -0.593      0.553   -1.19e+07    6.38e+06\n",
      "ProdComp_TF1 Films Production       5.961e+06   7.19e+06      0.829      0.407   -8.14e+06    2.01e+07\n",
      "ProdComp_TSG Entertainment          9.122e+05   7.01e+06      0.130      0.896   -1.28e+07    1.46e+07\n",
      "ProdComp_The Weinstein Company     -9.314e+05   5.49e+06     -0.170      0.865   -1.17e+07    9.83e+06\n",
      "ProdComp_Touchstone Pictures         1.59e+07   4.57e+06      3.483      0.001    6.95e+06    2.49e+07\n",
      "ProdComp_Universal Pictures         1.381e+07   2.82e+06      4.892      0.000    8.28e+06    1.93e+07\n",
      "ProdComp_Village Roadshow Pictures  7.873e+06   5.63e+06      1.399      0.162   -3.16e+06    1.89e+07\n",
      "ProdComp_Walt Disney Pictures       2.679e+07   4.66e+06      5.751      0.000    1.77e+07    3.59e+07\n",
      "ProdComp_Warner Bros. Pictures      1.684e+06   3.16e+06      0.533      0.594   -4.51e+06    7.88e+06\n",
      "ProdComp_Working Title Films        1.444e+06   6.35e+06      0.227      0.820    -1.1e+07    1.39e+07\n",
      "const                                3.72e+08   1.89e+08      1.969      0.049    1.65e+06    7.42e+08\n",
      "==============================================================================\n",
      "Omnibus:                      606.983   Durbin-Watson:                   1.992\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2441.537\n",
      "Skew:                           0.875   Prob(JB):                         0.00\n",
      "Kurtosis:                       6.884   Cond. No.                     2.54e+23\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 5.63e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "\n",
      "\n",
      "Regression Model Coefiicents and Importances\n",
      "\n",
      "OLS Coefficients\n",
      "\n",
      "\n",
      "Random Forest - Built-in Feature Importance\n",
      "\n",
      "\n",
      "Permutation Importance\n",
      "\n",
      "\n",
      "Part 6 - Classification Model-Based Insights - WIP\n",
      "\n",
      "Random Forest Classifier - BuiltIn Feature  Importances\n",
      "\n",
      "\n",
      "Random Forest Classifier -  Permutation Improtances\n",
      "\n",
      "\n",
      "Random Forest - SHAP Summary Plot\n",
      "\n",
      "\n",
      "Summary\n",
      "\n",
      "Coming soon!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readme = articles[0]\n",
    "print(readme.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2916ba-acd5-4c7c-9337-26cb870634e8",
   "metadata": {},
   "source": [
    "# AAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3505b11a-daa8-4868-a65b-2199477bcddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR AAB approach\n",
    "profile = \"https://github.com/jirvingphd\"\n",
    "class_=\"js-pinned-items-reorder-container\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e230ed46-ca1c-4bb7-9bc6-4878783b4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(profile)\n",
    "soup = bs4.BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8279361-3294-4b1e-bebf-e030b8f30772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pins  = soup.find_all(class_=class_)\n",
    "len(pins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07377802-ab8a-40fb-bbf0-cfa6a3a6f4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Pinned\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "how-to-make-successful-movies\n",
      "Public\n",
      "\n",
      "\n",
      "\n",
      "        Exploration of successful movies using IMDB, TMDB API, MySQL Database Construction, Hypothesis Testing, And Regression Modeling\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "Jupyter Notebook\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            1\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "iowa-prisoner-recidivism-mod-3-project\n",
      "Public\n",
      "\n",
      "\n",
      "\n",
      "          Forked from learn-co-students/dsc-3-final-project-online-ds-ft-021119\n",
      "\n",
      "\n",
      "        Classifying which released prisoners in Iowa will return to a life of crime using Next-Gen Gradient Boosted Trees \n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "Jupyter Notebook\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            1\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            1\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "how-to-spot-a-russian-troll-tweet-mod-4-project\n",
      "Public\n",
      "\n",
      "\n",
      "\n",
      "          Forked from learn-co-students/dsc-4-final-project-online-ds-ft-021119\n",
      "\n",
      "\n",
      "        How to Spot a (Russian) Troll - Classifying Troll Tweets vs Authentic Tweets\n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "Jupyter Notebook\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            1\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "predicting-the-SP500-using-trumps-tweets_capstone-project\n",
      "Public\n",
      "\n",
      "\n",
      "\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "pinned_repos = pins[0]\n",
    "print(pinned_repos.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b390ad4a-e09a-4a9c-98be-93544435e3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = pinned_repos.find_all('a', href=True)\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "436a7f21-2537-46ee-bdd4-33cffd394f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"mr-1 text-bold wb-break-word\" data-hydro-click='{\"event_type\":\"user_profile.click\",\"payload\":{\"profile_user_id\":44880739,\"target\":\"PINNED_REPO\",\"user_id\":null,\"originating_url\":\"https://github.com/jirvingphd\"}}' data-hydro-click-hmac=\"e294bdc3208f2eab7fd3ec5ff1600083f0fc9ff2cc544f74a29ddfd4f678545d\" href=\"/jirvingphd/how-to-make-successful-movies\">\n",
       "<span class=\"repo\" title=\"how-to-make-successful-movies\">how-to-make-successful-movies</span></a>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1eb6a24a-bd54-4914-aa4b-ddd6b2445663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"mr-1 text-bold wb-break-word\" data-hydro-click='{\"event_type\":\"user_profile.click\",\"payload\":{\"profile_user_id\":44880739,\"target\":\"PINNED_REPO\",\"user_id\":null,\"originating_url\":\"https://github.com/jirvingphd\"}}' data-hydro-click-hmac=\"e294bdc3208f2eab7fd3ec5ff1600083f0fc9ff2cc544f74a29ddfd4f678545d\" href=\"/jirvingphd/how-to-make-successful-movies\">\n",
       "<span class=\"repo\" title=\"how-to-make-successful-movies\">how-to-make-successful-movies</span></a>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = links[0]\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "76d5d587-9670-4430-911c-aa0f47a622f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['mr-1', 'text-bold', 'wb-break-word'],\n",
       " 'data-hydro-click': '{\"event_type\":\"user_profile.click\",\"payload\":{\"profile_user_id\":44880739,\"target\":\"PINNED_REPO\",\"user_id\":null,\"originating_url\":\"https://github.com/jirvingphd\"}}',\n",
       " 'data-hydro-click-hmac': 'e294bdc3208f2eab7fd3ec5ff1600083f0fc9ff2cc544f74a29ddfd4f678545d',\n",
       " 'href': '/jirvingphd/how-to-make-successful-movies'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c6b0c4c-661e-4519-8cde-c7c98a5c1c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/jirvingphd/how-to-make-successful-movies'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relative link\n",
    "rel_link = link['href']\n",
    "rel_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff4e5916-4b8c-4b3a-aac6-d449d9c629d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.github.com/jirvingphd/how-to-make-successful-movies'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "## set base url and test join\n",
    "base_url = \"http://www.github.com\"\n",
    "# abs_link = base_url + tag.attrs['href']\n",
    "abs_link = urljoin(base_url,rel_link)\n",
    "abs_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd25f0-6636-47d9-8825-07d1f2f479f8",
   "metadata": {},
   "source": [
    "# Putting it all together (AAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9884a-2186-4cba-9ec5-da63321cfe94",
   "metadata": {},
   "source": [
    "## Generate List of Repo Links from Profile Pins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c08fd2c6-f35e-4bbb-ae62-0a8b0d444471",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = \"https://www.github.com/Caellwyn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ec8fd08f-f369-4954-bf1f-8aa9c1ea88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for pins\n",
    "class_=\"js-pinned-items-reorder-container\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7103caed-9e24-4e4f-bf11-7ce1fcf5e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(profile)\n",
    "soup = bs4.BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e5a09f19-bbec-440a-b195-84318a4cfee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pins  = soup.find_all(class_=class_)\n",
    "len(pins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "37374cb1-7c9e-4f84-a69a-a3611ae3dabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinned_repos = pins[0]\n",
    "len(pinned_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7b3ee977-8883-49da-a18d-63316d3e9e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = pinned_repos.find_all('a', href=True)\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d253aa63-dc85-496f-8495-bbbec4821cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "base_url = \"https://www.github.com\"\n",
    "repo_links = []\n",
    "\n",
    "# saving list of absolute links\n",
    "\n",
    "for link in links:\n",
    "    # relative link\n",
    "    rel_link = link['href']\n",
    "    abs_link = urljoin(base_url,rel_link)\n",
    "    \n",
    "    # remove stars and forks\n",
    "    if abs_link.endswith('stargazers') | abs_link.endswith('forks'):\n",
    "        pass\n",
    "    else:\n",
    "        repo_links.append(abs_link)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9b3c3e8d-bc40-4bbf-86d0-7a5f0af7a9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.github.com/Caellwyn/ou_student_predictions',\n",
       " 'https://www.github.com/Caellwyn/product-flexible-twitter-sentiment-analysis',\n",
       " 'https://www.github.com/Caellwyn/Seattle-Home-Sales',\n",
       " 'https://www.github.com/Caellwyn/chat-with-a-philosopher',\n",
       " 'https://www.github.com/Caellwyn/pet-predictor',\n",
       " 'https://www.github.com/learn-co-curriculum/streamlit-image-classifier-demo',\n",
       " 'https://www.github.com/Violet-Spiral/covid-xprize']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_links#[1].endswith('stargazers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ca226-1323-4a95-b1a9-d6328aec3c98",
   "metadata": {},
   "source": [
    "## Scrape Repo README's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "149e6041-5394-45bd-b13a-647f76a1de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0efcbf3b-eb98-4d9a-90b8-f0c6613b7ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.github.com/Caellwyn'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ccc2b6a-d807-4ffd-8f5c-7bda5a756d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile in repo_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a0aa01ef-9e98-4a0c-8dea-5c163e90ee27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ou_student_predictions'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_links[0].replace(profile,'').replace('/','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c5b48a8e-3335-4e87-9cdb-cd899aadc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "READMES={}\n",
    "\n",
    "for repo in repo_links:\n",
    "\n",
    "    # get response\n",
    "    response = requests.get(repo)\n",
    "    # make a beautiful soup\n",
    "    soup = bs4.BeautifulSoup(response.content)\n",
    "    \n",
    "    articles = soup.find_all('article')\n",
    "    try:\n",
    "        readme = articles[0]\n",
    "        \n",
    "        key = repo.replace(base_url+'/','')#.replace('/','')\n",
    "        READMES[key] = readme.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(e)\n",
    "        print(repo)\n",
    "    sec_sleep = np.random.choice([1.9,1.2, 1.34,1.1,0.9])\n",
    "    time.sleep(sec_sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5cc5eaaf-50e7-4d89-b192-f6448d892586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Caellwyn/ou_student_predictions', 'Caellwyn/product-flexible-twitter-sentiment-analysis', 'Caellwyn/Seattle-Home-Sales', 'Caellwyn/chat-with-a-philosopher', 'Caellwyn/pet-predictor', 'learn-co-curriculum/streamlit-image-classifier-demo', 'Violet-Spiral/covid-xprize'])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "READMES.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e7e1568c-9e0a-4d88-be9f-4699e65f3f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Student Success Using Virtual Learning Environment Interaction Statistics\n",
      "\n",
      "Image by Goran Ivos, courtesy of Unsplash\n",
      "Introduction:\n",
      "The Problem\n",
      "Online learning has been a growing and maturing industry for years now, and in a way, the internet has always been about learning.  Khan Academy was, to me, a quintessential step in self-driven learning and massive open online courses (MOOCs) like Coursera create an even more structured approaches to online, self-driven learning.  I took my first online course on coding fundamentals from edX in 2013 and started my data science journey in 2019 on Coursera.\n",
      "While these are an amazing resources, they also have high dropout and failure rates.  Many students, accustomed to the accountability features in traditional face-to-face learning, struggle with the openness of these systems and the lack accountability.  In self-driven learning, it can be easy to get lost or give up.\n",
      "The Solution\n",
      "I set out to find ways to use data to help students succeed by identifying those who are in danger of failing or withdrawing early.  Many students in need of special intervention to improve their chances of success can be identified by the way they interact with the online learning system itself.\n",
      "The goal of this project is to model student interactions with an online learning environment to predict whether they will ultimately pass or fail a course early enouth for intervention to be effective.  My model captures the underlying patterns of behavior that predict the success of students in any self-paced online learning environment.\n",
      "In order to simulate this kind of prediction, I fit a model on chronologically earlier cohorts of students, their behavior and outcomes, and then validate my model on the latest cohorts present in the dataset.\n",
      "Directory:\n",
      "Report Notebook\n",
      "Exploratory Data Analysis\n",
      "Shallow Model Development\n",
      "Deep Model Development\n",
      "Data\n",
      "Collection of Model Candidates\n",
      "Various Assorted Charts\n",
      "Working Environment\n",
      "Presentation\n",
      "The Data\n",
      "I used a dataset from Open University based out of Great Britain.  They are an entirely online university offering accredited degrees to distance learners.  They released anonymized data from seven courses, three from the social sciences and four STEM courses, from the academic years 2012/2013 and 2013/2014.  These courses are repeated twice per year and each repetition is call a 'presentation'. Not all presentations of each course are included in the dataset.  To learn more about how the data was selected and anonymised, please visit this US National Institute of Health site that holds the data for scientific inquiry.\n",
      "The data is contained in this zip file and can be extracted and concatenated with data = load_OU_data() function in the src.functions module.  You can use the argument prediction_window= to specifiy either a number of days as an integer or a proportion of the data as a float to adjust when in the courses you want to end the prediction window and have the model provide it's predictions.  If run from the '/report' or '/notebooks' folder that function will do all the loading and merging for you.\n",
      "This dataset contains regional, demographic, and personal data about students, but I was only interested in data about how they interacted with the learning environment.  In the studentVle.csv file available in the anonymiseData.zip file the university provided information about each activity that each student interacted with, including the date relative to course start and number of clicks on that activity.\n",
      "For Statistics on Demographic Data, See This Associated Dashboard\n",
      "Features for Predictive Modeling\n",
      "I did not use most of the data in the dataset as it was very specific to those students and the region.  My goal is the capture the relationship between student behavior and success, not their background, gender, disability, or socioeconomic situation.  These were also not highly correlative to student success when I explored them.\n",
      "Instead I used 6 features that a virtual learning environment would be able to collect anonymously:\n",
      "Features:\n",
      "Activity Statistics\n",
      "\n",
      "Number of total clicks during course.\n",
      "Number of total activities completed\n",
      "Number of days worked\n",
      "\n",
      "It was the last statistic that I felt would be uniquely predictive.  I spent 14 years teaching students and one thing I learned from research and experience is that spreading learning out over more days, rather than cramming more learning on fewer days, helps with persistence and with learning retention.  It helps with persistence because it causes the student to form a habit of learning and we are creatures of habit.  Cramming a lot of learning into fewer days is also more exhausting and less pleasant, creating a sense of aversion to the act.  Frequent reinforcement of learning also serves to reinforce neuronal axons that are the physical manifestation of learning, which makes learning last longer.  I'll show what I found on this later on in this notebook.\n",
      "The next statistics I pulled to use to train my model were on assessments.\n",
      "Assessment Statistics\n",
      "\n",
      "Average assessment score\n",
      "percentage of assessments completed.\n",
      "\n",
      "The first was an obvious choice, and the second is meaningful because these courses were in part self-paced.  I found that doing more assessments earlier was correlated to greater success, as you will see in the data exploration section later.\n",
      "Correlations in the Data\n",
      "Repeat Students\n",
      "After preparation I had data on 22,424 individual students who registered for classes 24,743 times.  The data from the Open University dataset was organized into registrations so that is how I organized my data.  I recognize that those 2,319 registrations that represent multiple registrations by the same student violates the independence of observations, and would invalidate inferential modeling.  When I do that kind of analysis I will remove the duplicates.\n",
      "Courses (code_module)\n",
      "Unfortunately, I had bigger problems.  The data is divided into 7 different courses with different numbers of activities, students, success rates, and average assessment scores.  I want my model to be generalizable to many online courses outside of this dataset and to capture underlying relationships between student behavior and their success in a course.\n",
      "\n",
      "You can see above that there are significantly different distributions of assessment scores for different courses.\n",
      "\n",
      "Similarly, you can see that students engaged different numbers of activities depending on the course.\n",
      "\n",
      "Finally we see a worryingly high difference in completion rates between the courses.\n",
      "These correlations would make my model accuracy vary by course, my model's ability to predict success specific to these courses, rather than applying broadly.\n",
      "In order to correct this problem, I took two steps:\n",
      "\n",
      "I scaled the features of each course separately.  Activities completed, assessment scores, etc. are now all relative to the mean for the individual courses.\n",
      "I used Imbalanced Learn's SMOTE to create synthetic data for the minority class in each course to balance the success rates.  This method creates data which is statistically similar to the data in that class using a K-Nearest Neighbors approach.\n",
      "\n",
      "The data features and targets were then no longer correlated to the courses.  My model could focus on the behavior itself, relative to how other students are performing in the same course.\n",
      "The downside is that I cannot use presentations in the test set, the 2014J presentations, for testing if they are the only presentation available.  My preprocessing needs previous examples of a course in order to properly scale the features.  In other words I can't do predictions on courses that are being taught for the first time, or that I don't have historical data for.\n",
      "\n",
      "The above chart shows correlations between each variable my model uses for predictions, as well as course (code_module) and final_result.  If you look along the row for code_module you will see that, for the most part, the correlations between courses and features have been removed, and the correlation between courses and final_result has been completely removed.\n",
      "If you look down the final result row you can also see how strongly each of the aggregate features I extracted are to the final result.\n",
      "\n",
      "Days Studied: .35\n",
      "Activities Engaged: .31\n",
      "Total Clicks: .27\n",
      "Assessments Completed: .36\n",
      "Average Assessment Score: .46\n",
      "\n",
      "I was very excited to see days_studied with such a high correlation.  My hypothesis from the start was that spreading learning out over more days would improve persistence and learning and I see validation here.\n",
      "You can also see that each of these statistics are also highly correlated to each other, especially days_studied, activities_engaged, and total_clicks.  These variables all represent how much work a student puts in and it's very reasonable that they would correlate.  The count and average score of assessments are also correlated, but less strongly, which is interesting to me. The number of assessments completed varies by student because students can work ahead.\n",
      "These correlations between predictor variables are a problem if I try to do inferential statistics on these features.  I would not be able to untangle which variables are actually affecting the results and how, since one changes the other.  If I wanted to dig deeper into inference I would have to choose just one of these features to explore and maybe bring back the demographic features that I dropped.\n",
      "Data Preparation\n",
      "I've discussed some of the preparation I did, but here is a more comprehensive description:\n",
      "\n",
      "\n",
      "I limited the data to what makes sense for the prediction window, or the points in the course I want to make my predictions on.  I won't try to predict the outcomes of students who have already withdrawn, and I a won't use any data that I would not yet have by that point in the course.\n",
      "\n",
      "\n",
      "My data target classes are 'Distinction', 'Pass', 'Fail', and 'Withdrawn'.  However, my predictor only predicts whether a student needs intervention or not.  Those passing or passing with distinction do not need interventions, while those failing or withdrawing do.  I will combine the first two into a new class 'No Intervention' and the second two into 'Needs Intervention'.  This makes my target binary, easier to predict and easier to evaluate.\n",
      "\n",
      "\n",
      "I split the data into training data and testing data to validate my models and identify and prevent overfitting.  Specifically I held all of the chronologically latest presentations, 2014J as a final test set and then used each presentation in befor that, in turn, as validation sets in my cross validation function.  My model was always being tested on a presentation of a course that it was not trained on.  I did this because that is the purpose of my model, to make predictions on a current cohort of students.  I want a model that can accurately predict for groups it has not yet seen.  If I were to randomly choose students across cohorts, or presentations, and then test on different students from the same cohorts it trained on, the model would suffer data leakage from variations between presentations.\n",
      "\n",
      "\n",
      "I scaled all the data course by course.  The CourseScaler class will fit on each individual course in the training set of observations, as indexed by the 'code_module' variable.  It will take the mean and standard deviation for each feature for each course from the training data and use these means and standard deviations to scale ((feature-mean)/standard deviation) features for both the training and the testing data.  This will remove the as much correlation between code_module and the activity and assessment statistics as possible.\n",
      "\n",
      "\n",
      "I used the smotecourses() function to apply imblearn.over_sampling.SMOTE to the training data and create synthetic data within the minority class to balance the classes.  Each course in the training set has equal number of students needing intervention and not needing interventions in the training data.  This will remove the correlation between code_module and final_result as much as possible.\n",
      "\n",
      "\n",
      "Once those are done, I drop the code_module column because I don't want my model considering that in its predictions.\n",
      "\n",
      "\n",
      "Model Development\n",
      "Custom Tools\n",
      "The preprocessing needs for this project are unique and don't work well with off the shelf Sci-Kit Learn and Imbalanceed Learn pipelines, cross-validation, and grid search tools.  My preprocessing steps need the code_module feature in order to scale and balance each course separately, but that feature should not be passed to the model.  I also wanted to cross-validate by presentation rather than by randomly assembled folds.  Because of this I had to code my own versions of each of these that would fit a scaler to each course in the training set, use that fit scaler to scale both the training and test set, and then use the Imbalanced Learn SMOTE over-sampler to balance the classes in only the training set.  I also needed custom gridsearch and cross-validation tools to properly apply the proprocessing to prevent data leakage and to cross-validate by presentation rather than by randomly sampled observations.  Once I had these coded, my model development could proceed.\n",
      "Logistic Regression\n",
      "I used a logistic regression model for a baseline model.  A logistic regresson model seeks the best fit line to model the linear relationship between the predictor variables, X, and the target variable, y, which is a linear regression. It then applies a sigmoid function to that line to assign probabilities that each observation belongs in one class or the other.  For my purposes, if the probability of an observation belonging a class is greater than .5, then I will predict that it belongs to that class.  A true baseline would be a randomly guessing model with an accuracy equal to the rate of class imbalance, or, in this case, about 34%.  My hyperparameter-tuned logistic regression model was able to achieve a 74% accuracy on the 2014J presentations and 79% average cross-validation scores.  There is some amount of variance in its performance on each fold, from 69% to 83%, and was 70% accurate on one presentation of module GGG and 80% accurate on another.  This indicates to me that some cohorts are harder to predict that others.  The lower accuracy on the hold-out dataset may indicate that it was generally a harder year for prediction.  In my final model I will explore accuracy across modules within the holdout set, and I find a similar amount of variation across modules, with no apparent pattern between social science and STEM modules\n",
      "This and most of my shallow models tend to overpredict that students will pass the course, with lower accuracy on students who will not.\n",
      "K-Nearest Neighbors\n",
      "A KNN model uses a tuneable distance metric to determine the distance between observations in n-dimensional space, where n is the number of independent variable features in the training set.  The new observation is classified according to the classes of the K (a tuneable hyper-parameter) nearest training observations.  This classification technique hypothesizes that observations of the same class will cluster together in the n-dimensional space.\n",
      "My KNN model, after hyperparameter tuning, achieved almost identical results as the logistic regression model.  One benefit of both models, however, is that they train and predict quite quickly.  KNN is a great model to use when you have a low number of features because when classes do cluster together, they do so more tightly in lower dimensional space.\n",
      "Decision Tree\n",
      "The logistic regression model was pretty successful, being twice as accurate as random guessing, but I thought perhaps the problem was not as linear as that model likes.  A decision tree is a promising candidate for a less linear relationship between independent and dependent variables because it does not assume the independence of the features or linear relationship between features and target. Instead it seeks to find the best way to divide and subdivide the data in a tree structure based on the values of different variables.  Once the tree is built, predictions are made by sending an observations down the tree on a path to the predicted class as it reaches each split in the tree is sent in one or the other direction.  You can think of it like a deterministic Pachinko machine.\n",
      "This model averaged 77% on cross-validation scores, and 73% on the validation set.  Apparently linearity of relationships between the variables is not the only hurdle to overcome in modeling student success.\n",
      "Random Forest\n",
      "This is an interesting extension to the decision tree model.  It creates a whole forest of decision trees and trains each one on a subset of the data and a subset of the features.  This is a technique called bagging, or Boostrap AGGregation (check the link for more on this).  It works on the principle that a bunch of bad predictors, on average will be more accurate than one good predictor.  This worked for Francis Galton in guessing the weight of an ox, maybe it will work here!\n",
      "In fact, this bagging strategy performed a little better than the single decision tree.  It showed a 78% accuracy across training folds and 73% on the testing presentations.  Specifically it was a little more accurate in predicting which students needed intervention (71% vs 69%).  This model was less prone to overpredicting students to pass.\n",
      "eXtreme Gradient Boosting\n",
      "XGBoost models have gained a lot of popularity recently and won a lot of Kaggle competitions.  It uses another popular idea called boosting.  That's a pretty involved wikipedia article, but the TLDR is that it's a ensemble method like random forest, but whereas random forest trains many trees in parallel and takes the aggregate of their predictions, boosting stacks the trees on top of each other.  Each one tries to improve on the one before it by predicting the previous one's mistakes.  I think of it as like a line of poor students each grading the last one's paper, which is an analysis of the previous one's paper, etc.  Each one gets a lot wrong, but something right so the right answers percolate forward and some of the wrong answers get corrected at each step.\n",
      "\n",
      "The XGBoost model again performed similarly to the other shallow models with an average accuracy of 77% across validation folds and 74% accuracy on the 2014J modules.\n",
      "Final Model: Deep Neural Network\n",
      "\n",
      "I had the best luck implementing a 10 layer deep neural network for predictions.  This model predicted student success at 77% accuracy on the validation set and 76% on the hold-out set.  I did not use cross validation with the dense model, as this would have proven unfeasible given the training time and the number of cross validation folds I was using with other model.  Instead I reserved 20% of the training set, presentations before October of 2014, for validation and conducted final testing on the 2014J modules.  It continues, however to have a lot of variance in the accuracy across modules, and to overpredict student success.\n",
      "In the end my models did not generalize across courses as well as I had hoped, however they can all predict student success with a minimum of 67% on any of the presentations of any of the modules in this dataset.  This indicates to me that there is some validity in my approach, using data about how students interact with their virtual learning environments to predict their success using stastistical modeling.  The fact that the variance between presentations and modules is only about 20% shows that some patterns hold true between social sciences and STEM courses and between years and cohorts.\n",
      "Below you can find the confusion matrices for the October 2014 cohorts in each module in the dataset and the associated overall accuracy for that cohort.  You can see that, with the exception of Module AAA, which has the least number of students, this deep, dense model predicted with between 73% and 78% accuracy for each module.  However, for modules CCC and DDD it drastically overpredicted that students would pass the course.  Even at 50% accuracy in module DDD, however, it was better than random, which would have been about 33% for this unbalanced dataset.\n",
      "\n",
      "Error Analysis\n",
      "Patterns in the Predictions\n",
      "\n",
      "\n",
      "\n",
      "There are significant differences in the true means of each of these variables between those who pass the courses and those who don't.  The black lines represent standard deviations within each group.\n",
      "\n",
      "\n",
      "This model tended to exaggerate the difference it seemed to split on.  The average values in the predicted success groups are all a little higher than the average of the true success groups and the average failing values a little lower.\n",
      "\n",
      "\n",
      "The means of the predictor variables for the students that the model misclassified show they are true outliers for their class.  Their numbers are much closer to their opposite class, and it makes sense why the model misclassified them.  The information present in the model may simply be insufficient to accurate predict the outcomes for the students, and the model error may represent irreducible error with the data used.  It's possible that these are the cases where demographic data may tell a deeper story, but also may just be unique circumstances.\n",
      "\n",
      "\n",
      "Future deployment\n",
      "This model can be useful in flagging students for intervention in online courses.  However, preprocessing data correctly is crucial for model accuracy.  In order to properly scale new data the CourseScaler transformer would need to be fitted on at least one entire course worth of data, or would at least need to be provided with the mean and standard deviation of each feature for that course.  This model does not lend itself to web deployment, but could be integrated into the backend analytics of an online education provider to help target students for support.\n",
      "Summary:\n",
      "I built a dense neural network that could predict student success in an online course with, on average, 76% accuracy.  The variation in accuracy between presentations in both the cross-validation presentations and the testing presentations was significant, however, ranging from 67% to 78%.  This model is somewhat generalizable across courses with different student graduation rates, different course requirements.  However, in some cases it struggles to predict which students will need intervention.\n",
      "Next Steps:\n",
      "\n",
      "\n",
      "Overall accuracy may not be the best metric to train or evaluate my models on.  Evaluating using a precision-recall area-under-the-curve graph might lend insight into how different probability thresholds could improve the accuracy of this model or balance the accuracy of predictions across classes.\n",
      "\n",
      "\n",
      "The dense modeling process has room for more experimentation with different regularization techniques and model structures.\n",
      "\n",
      "\n",
      "I would love to try this model and preprocessing routine out on new datasets to see how well it generalizes.\n",
      "\n",
      "\n",
      "I could try removing some of the outliers or scaling my data in different ways.\n",
      "\n",
      "\n",
      "Finally, I could dig deeper into the errors my model is making to see if I can see what trips it up. What are the commonalities among students that are misclassified\n",
      "\n",
      "\n",
      "References:\n",
      "Data Sourcing: Kuzilek J., Hlosta M., Zdrahal Z. Open University Learning Analytics dataset Sci. Data 4:170171 doi: 10.1038/sdata.2017.171 (2017).\n",
      "1 https://nces.ed.gov/fastfacts/display.asp?id=80\n",
      "2 https://journals.sagepub.com/doi/pdf/10.1177/2158244015621777#:~:text=Online%20courses%20have%20a%2010,Smith%2C%202010)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(READMES['Caellwyn/ou_student_predictions'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
